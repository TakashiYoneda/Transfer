0. Instance selection: 
	- Focus on essenial core 
	- Learn with smaller and more effective data than huge data
	- Reduction of dimensionality of the data (Irrelevant & Redundant)
		--> learning algo operates faster and more efficiently

1. Clean Data
	>>> Irelevant info will blur essential features
	>>> High-Polynominal model: naive to overfit to "Noise"
		- Choose representing instance group among class samples
		- Outlier elimination:
			- Classification/Regression 
			- Decision Tree (Entropy based method)
			- SVM (distance, dencity)
			- Max-Min threshold
			- Unsupervised Classifier (???)
		
		- Irrelevant features elimination: 
			- Color: Gray scale + Filters for edge detection
			- Object detection & Background elimination

2. Modify Imbalance
	>>> Bias: Heavily represented class
	>>> Overfit: small ratio classes (too specifically)
		- Remove training examples of the over represented class
		- Duplicate training examples of the under represented class
			- Histogram of classes

3. Normalize Data
	>>> Big scale data will dominate influence
			- z-score

4. Robustness
	>>> Recognition of object is limited aspect
			- Round, V/H shift, Partial Recognition?, ,,,
			- (Missing Data handling: identify, eliminate how?)

5. Cross-Validation: 
	>>> Avoid huge computational calculation
		- Random sampling
		- Stratified sampling
			set higher frequency of selection for minor classes

6. Feature Construction/Transformation
	>>> 
		- GALA algo: 
			throughtout the course of building a decision tree classifier
		- Construct new features from the basic feature set
			Transformed features generated by feature constuction may
			provide a better discriminative ability than 
			the best subset of given features



~ Preprocessing
I. Clean
	1. Mean Subtraction:
		RGB: each color,   One dim;
		X -= np.mean(X, axis = 0) for all individual data

	2.Normalization:
		RGB: each color, One dim:
		X -= np.mean(X, axis = 0) then X /= np.std(X, axis = 0) 
			for all individual data

	3.PCA & Whitening: (not used in CNN)

	>> Common pitfall.
		any preprocessing statistics (e.g. the data mean) must
		only be computed on the training data, and then
		applied to the validation / test data.

		E.g. computing the mean and subtracting it from 
		every image across the entire dataset and then 
		splitting the data into train/val/test splits would be a mistake.

		Instead, the mean must be computed only over the training data
		and then subtracted equally from all splits (train/val/test).

  
	- check labeled data (# variance)
	  -> Outlier detection, Redundancy elimination, Normalization,

	- check labeled data (# each categories)  -> Item size balance

 
Hyper-parameters:
	W, b,


II. Weight Initialization

	Pitfall: all zero initialization, too small initialization (Gradient Vanish)

	Small random number:
		W = np.random.randn(n) / sqrt(n)  (n: # of inputs)  
		  = np.random.randn(n) * sqrt(2.0/n)   in case of Relu NN

		B = zero ok


	learning_rate = 10 ** uniform(-6, 1).
	Regularization_strength = 10 ** uniform(-6, 1).

	Epochs: (no need to cover all samples)
	Mini-batch size(*)

III. Batch normalization:


VI. Network Size/Layers
	larger is preferable than smaller network, if takes below methods:
	  L2 regularization, dropout, input noise ,  regularization(λ)

	Activation function:
	~ Sigmoid: recently out of favor 
	   (kill gradient <-5 5<, non-zero centered: zigzag in G-update in weights)
	~ Tanh: (kill gradient <-2 2<) better than sigmoid
	~ Relu: 6x faster learning than tanh, if learning rate is high, 
		40% of neurons can be dead
	~ Leaky Relu: merit is not clear now
	~ Maxout: max( Relu, Leaky Relu)  more parameters and calculation
	
	CNN(Kernel size, Stride, padding)
	Pooling

	Dropout:  0.5
                                                                                   
	Loss function
	GD evaluation

	The two recommended updates:
	~ either SGD+Nesterov Momentum  or  Adam.

	L2 regularization: GD update  W += -lambda * W



VIII. Sanity check:
	20 samples w/o regularization: loss = 0

IX. Search for good hyperparameters with random search (not grid search):

	Stage your search

	from coarse (wide hyperparameter ranges, 20 samples, only for 1 epoch), 
	then perform a narrower search with 5 epochs,
	to fine (narrower rangers, training for many more epochs)

	Step decay: 
	Reduce the learning rate by some factor every few epochs.
	Typical values might be reducing the learning rate 
	by a half every 5 epochs, or by 0.1 every 20 epochs. 
	These numbers depend heavily on the type of problem and the model.

  
                                                                                   

86 bio neurons, connected 10^14-10^15 synapses (1^12 = 1 tio, 10^9 = 1 bio)

1.2 million samples

~ mini batch GD  (SGD)
256 batch size = 0.02%-0.1% of total sample size
Mini-batch size = power’s’ of 2 (32, 64, 128, 256,,,)

To avoid redundancy evaluation (if each 1,000 sample -> 1,200 copied samples -> 1.2 mio total samples)


